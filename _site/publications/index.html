<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Publications - Hongyan Chang</title>
  <meta name="description" content="Hongyan Chang">
  <link rel="stylesheet" href="/https://www.comp.nus.edu.sg/~hongyan/assets/main.css">
  <link rel="canonical" href="http://localhost:4000https://www.comp.nus.edu.sg/~hongyan//publications/">
  <link rel="shortcut icon" type ="image/x-icon" href="http://localhost:4000https://www.comp.nus.edu.sg/~hongyan//hongyan.png">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">

  <link rel="preconnect" href="https://player.vimeo.com">
  <link rel="preconnect" href="https://i.vimeocdn.com">
  <link rel="preconnect" href="https://f.vimeocdn.com">




<script>
MathJax = {
    tex: {
    inlineMath: [ ['$', '$'], ['\\(', '\\)'] ],
    tags: 'ams'  // should be 'ams', 'none', or 'all' }. This line makes the equation numbering and labeling work
    }, 
    svg: {
    fontCache: 'global'
    }
};
</script>
<script
    type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script> 

</head>


  <body>

    <!-- Google Tag Manager (noscript) -->
<noscript><iframe src="https://www.googletagmanager.com/ns.html?id="
height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
<!-- End Google Tag Manager (noscript) -->

<nav class="navbar sticky-top navbar-expand-md navbar-dark bg-dark">
    <a class="navbar-brand" href="http://localhost:4000https://www.comp.nus.edu.sg/~hongyan//">
     <img src="http://localhost:4000https://www.comp.nus.edu.sg/~hongyan//hongyan.png" width="30" height="30" style="margin-right:5px" class="d-inline-block align-top" alt="">
      Hongyan Chang
    </a>
    <button class="toggler navbar-toggler collapsed" type="button" data-bs-toggle="collapse" data-bs-target="#navbarColor02" aria-controls="navbarColor02" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>

    <div class="collapse navbar-collapse" id="navbarColor02">
        <div class="navbar-nav mr-auto">
            <a class="nav-item nav-link" href="http://localhost:4000https://www.comp.nus.edu.sg/~hongyan//">Home</a>
            
        </div>
    </div>
</nav>


    <div class="container-fluid">
      <div class="row">
        <div id="gridid" class="col-sm-12 col-xs-12">
  <style>
.jumbotron{
    padding:3%;
    padding-bottom:10px;
    padding-top:10px;
    margin-top:10px;
    margin-bottom:30px;
}
</style>

<div class="jumbotron">
  <h3 id="refereed-journal-articles">Refereed journal articles</h3>
  <ol class="bibliography" reversed="reversed"><li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px
}

pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify"><span id="chang2024watermark">Chang, H., Hassani, H., &amp; Shokri, R. (2024). Watermark Smoothing Attacks against Language Models. <i>ArXiv Preprint ArXiv:2407.14206</i>.</span></div>

<!-- You can use the below to make your name bold -->
<!-- <span id="chang2024watermark">Chang, H., Hassani, H., &amp; Shokri, R. (2024). Watermark Smoothing Attacks against Language Models. <i>ArXiv Preprint ArXiv:2407.14206</i>.</span></div> -->







<a href="https://arxiv.org/abs/2407.14206" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>







<button class="btn btn-warning btm-sm" onclick="toggleAbstractchang2024watermark()">ABSTRACT</button>



<button class="btn btn-danger btm-sm" onclick="toggleBibtexchang2024watermark()">BIB</button>





<div id="achang2024watermark" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@article{chang2024watermark,
  title = {Watermark Smoothing Attacks against Language Models},
  author = {Chang, Hongyan and Hassani, Hamed and Shokri, Reza},
  journal = {arXiv preprint arXiv:2407.14206},
  date = {2024},
  doi = {2407.14206}
}
</pre>
</div>


<div id="bchang2024watermark" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>Watermarking is a technique used to embed a hidden signal in the probability distribution of text generated by large language models (LLMs), enabling attribution of the text to the originating model. We introduce smoothing attacks and show that existing watermarking methods are not robust against minor modifications of text. An adversary can use weaker language models to smooth out the distribution perturbations caused by watermarks without significantly compromising the quality of the generated text. The modified text resulting from the smoothing attack remains close to the distribution of text that the original model (without watermark) would have produced. Our attack reveals a fundamental limitation of a wide range of watermarking techniques.</pre>
</div>

<script>
function toggleBibtexchang2024watermark(parameter) {
    var x= document.getElementById('achang2024watermark');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractchang2024watermark(parameter) {
    var x= document.getElementById('bchang2024watermark');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px
}

pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify"><span id="chang2024efficient">Chang, H., Edwards, B., Paul, A., &amp; Shokri, R. (2024). Efficient Privacy Auditing in Federated Learning. <i>Usenix Security Symposium (USENIX)</i>.</span></div>

<!-- You can use the below to make your name bold -->
<!-- <span id="chang2024efficient">Chang, H., Edwards, B., Paul, A., &amp; Shokri, R. (2024). Efficient Privacy Auditing in Federated Learning. <i>Usenix Security Symposium (USENIX)</i>.</span></div> -->





<a href="http://localhost:4000https://www.comp.nus.edu.sg/~hongyan//papers/chang2024efficient.pdf" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>









<button class="btn btn-warning btm-sm" onclick="toggleAbstractchang2024efficient()">ABSTRACT</button>



<button class="btn btn-danger btm-sm" onclick="toggleBibtexchang2024efficient()">BIB</button>





<div id="achang2024efficient" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@article{chang2024efficient,
  title = {Efficient Privacy Auditing in Federated Learning},
  author = {Chang, Hongyan and Edwards, Brandon and Paul, Anindya and Shokri, Reza},
  journaltitle = {Usenix Security Symposium (USENIX)},
  date = {2024},
  file = {chang2024efficient.pdf}
}
</pre>
</div>


<div id="bchang2024efficient" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>We design a novel efficient membership inference attack to audit privacy risks in federated learning. Our approach involves computing the slope of specific model performance metrics (eg, model’s output and its loss) across FL rounds to differentiate members from non-members. Since these metrics are automatically computed during the FL process, our solution imposes negligible overhead and can be seamlessly integrated without disrupting training. We validate the effectiveness and superiority of our method over prior work across a wide range of FL settings and real-world datasets.</pre>
</div>

<script>
function toggleBibtexchang2024efficient(parameter) {
    var x= document.getElementById('achang2024efficient');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractchang2024efficient(parameter) {
    var x= document.getElementById('bchang2024efficient');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px
}

pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify"><span id="hampton2021black">Ganesh, P., Chang, H., Strobel, M., &amp; Shokri, R. (2023). On The Impact of Machine Learning Randomness on Group Fairness. <i>Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (FAccT) - Best Paper Award</i>.</span></div>

<!-- You can use the below to make your name bold -->
<!-- <span id="hampton2021black">Ganesh, P., Chang, H., Strobel, M., &amp; Shokri, R. (2023). On The Impact of Machine Learning Randomness on Group Fairness. <i>Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (FAccT) - Best Paper Award</i>.</span></div> -->









<a href="https://dl.acm.org/doi/abs/10.1145/3593013.3594116" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>





<button class="btn btn-warning btm-sm" onclick="toggleAbstracthampton2021black()">ABSTRACT</button>



<button class="btn btn-danger btm-sm" onclick="toggleBibtexhampton2021black()">BIB</button>





<div id="ahampton2021black" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@inproceedings{hampton2021black,
  title = {On The Impact of Machine Learning Randomness on Group Fairness},
  author = {Ganesh, Prakhar and Chang, Hongyan and Strobel, Martin and Shokri, Reza},
  booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency (FAccT) - Best Paper Award},
  date = {2023},
  urla = {https://dl.acm.org/doi/abs/10.1145/3593013.3594116}
}
</pre>
</div>


<div id="bhampton2021black" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>Statistical measures for group fairness in machine learning reflect the gap in performance of algorithms across different groups. These measures, however, exhibit a high variance between different training instances, which makes them unreliable for empirical evaluation of fairness. What causes this high variance? We investigate the impact on group fairness of different sources of randomness in training neural networks. We show that the variance in group fairness measures is rooted in the high volatility of the learning process on under-represented groups. Further, we recognize the dominant source of randomness as the stochasticity of data order during training. Based on these findings, we show how one can control group-level accuracy (i.e., model fairness), with high efficiency and negligible impact on the model’s overall performance, by simply changing the data order for a single epoch.</pre>
</div>

<script>
function toggleBibtexhampton2021black(parameter) {
    var x= document.getElementById('ahampton2021black');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstracthampton2021black(parameter) {
    var x= document.getElementById('bhampton2021black');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px
}

pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify"><span id="chang2023bias">Chang, H., &amp; Shokri, R. (2023). Bias Propagation in Federated Learning. <i>International Conference on Learning Representations (ICLR)</i>.</span></div>

<!-- You can use the below to make your name bold -->
<!-- <span id="chang2023bias">Chang, H., &amp; Shokri, R. (2023). Bias Propagation in Federated Learning. <i>International Conference on Learning Representations (ICLR)</i>.</span></div> -->









<a href="https://openreview.net/pdf?id=V7CYzdruWdm" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>





<button class="btn btn-warning btm-sm" onclick="toggleAbstractchang2023bias()">ABSTRACT</button>



<button class="btn btn-danger btm-sm" onclick="toggleBibtexchang2023bias()">BIB</button>





<div id="achang2023bias" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@article{chang2023bias,
  title = {Bias Propagation in Federated Learning},
  author = {Chang, Hongyan and Shokri, Reza},
  journaltitle = {International Conference on Learning Representations (ICLR)},
  date = {2023},
  urla = {https://openreview.net/pdf?id=V7CYzdruWdm}
}
</pre>
</div>


<div id="bchang2023bias" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>We show that participating in federated learning can be detrimental to group fairness. In fact, the bias of a few parties against under-represented groups (identified by sensitive attributes such as gender or race) can propagate through the network to all the parties in the network. We analyze and explain bias propagation in federated learning on naturally partitioned real-world datasets. Our analysis reveals that biased parties unintentionally yet stealthily encode their bias in a small number of model parameters, and throughout the training, they steadily increase the dependence of the global model on sensitive attributes. What is important to highlight is that the experienced bias in federated learning is higher than what parties would otherwise encounter in centralized training with a model trained on the union of all their data. This indicates that the bias is due to the algorithm. Our work calls for auditing group fairness in federated learning and designing learning algorithms that are robust to bias propagation.</pre>
</div>

<script>
function toggleBibtexchang2023bias(parameter) {
    var x= document.getElementById('achang2023bias');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractchang2023bias(parameter) {
    var x= document.getElementById('bchang2023bias');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px
}

pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify"><span id="chang2020privacy">Chang, H., &amp; Shokri, R. (2021). On the privacy risks of algorithmic fairness. <i>6th IEEE European Symposium on Security and Privacy (Euro S&amp;P)</i>.</span></div>

<!-- You can use the below to make your name bold -->
<!-- <span id="chang2020privacy">Chang, H., &amp; Shokri, R. (2021). On the privacy risks of algorithmic fairness. <i>6th IEEE European Symposium on Security and Privacy (Euro S&P)</i>.</span></div> -->









<a href="https://ieeexplore.ieee.org/abstract/document/9581219" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>





<button class="btn btn-warning btm-sm" onclick="toggleAbstractchang2020privacy()">ABSTRACT</button>



<button class="btn btn-danger btm-sm" onclick="toggleBibtexchang2020privacy()">BIB</button>





<div id="achang2020privacy" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@article{chang2020privacy,
  title = {On the privacy risks of algorithmic fairness},
  author = {Chang, Hongyan and Shokri, Reza},
  urla = {https://ieeexplore.ieee.org/abstract/document/9581219},
  journaltitle = {6th IEEE European Symposium on Security and Privacy (Euro S&amp;P)},
  date = {2021}
}
</pre>
</div>


<div id="bchang2020privacy" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>Algorithmic fairness and privacy are essential pillars of trustworthy machine learning. Fair machine learning aims at minimizing discrimination against protected groups by, for example, imposing a constraint on models to equalize their behavior across different groups. This can subsequently change the influence of training data points on the fair model, in a disproportionate way. We study how this can change the information leakage of the model about its training data. We analyze the privacy risks of group fairness (e.g., equalized odds) through the lens of membership inference attacks: inferring whether a data point is used for training a model. We show that fairness comes at the cost of privacy, and this cost is not distributed equally: the information leakage of fair models increases significantly on the unprivileged subgroups, which are the ones for whom we need fair learning. We show that the more biased the training data is, the higher the privacy cost of achieving fairness for the unprivileged subgroups will be. We provide comprehensive empirical analysis for general machine learning algorithms.</pre>
</div>

<script>
function toggleBibtexchang2020privacy(parameter) {
    var x= document.getElementById('achang2020privacy');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractchang2020privacy(parameter) {
    var x= document.getElementById('bchang2020privacy');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px
}

pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify"><span id="chang2020adversarial">Chang, H., Nguyen, T. D., Murakonda, S. K., Kazemi, E., &amp; Shokri, R. (2020). On adversarial bias and the robustness of fair machine learning. In <i>arXiv</i>.</span></div>

<!-- You can use the below to make your name bold -->
<!-- <span id="chang2020adversarial">Chang, H., Nguyen, T. D., Murakonda, S. K., Kazemi, E., &amp; Shokri, R. (2020). On adversarial bias and the robustness of fair machine learning. In <i>arXiv</i>.</span></div> -->







<a href="https://arxiv.org/abs/2006.08669" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>







<button class="btn btn-warning btm-sm" onclick="toggleAbstractchang2020adversarial()">ABSTRACT</button>



<button class="btn btn-danger btm-sm" onclick="toggleBibtexchang2020adversarial()">BIB</button>





<div id="achang2020adversarial" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@unpublished{chang2020adversarial,
  title = {On adversarial bias and the robustness of fair machine learning},
  author = {Chang, Hongyan and Nguyen, Ta Duy and Murakonda, Sasi Kumar and Kazemi, Ehsan and Shokri, Reza},
  journaltitle = {arXiv},
  date = {2020},
  doi = {2006.08669}
}
</pre>
</div>


<div id="bchang2020adversarial" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>Optimizing prediction accuracy can come at the expense of fairness. Towards minimizing discrimination against a group, fair machine learning algorithms strive to equalize the behavior of a model across different groups, by imposing a fairness constraint on models. However, we show that giving the same importance to groups of different sizes and distributions, to counteract the effect of bias in training data, can be in conflict with robustness. We analyze data poisoning attacks against group-based fair machine learning, with the focus on equalized odds. An adversary who can control sampling or labeling for a fraction of training data, can reduce the test accuracy significantly beyond what he can achieve on unconstrained models. Adversarial sampling and adversarial labeling attacks can also worsen the model’s fairness gap on test data, even though the model satisfies the fairness constraint on training data. We analyze the robustness of fair machine learning through an empirical evaluation of attacks on multiple algorithms and benchmark datasets.</pre>
</div>

<script>
function toggleBibtexchang2020adversarial(parameter) {
    var x= document.getElementById('achang2020adversarial');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractchang2020adversarial(parameter) {
    var x= document.getElementById('bchang2020adversarial');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li>
<li><style>
.btn{
    margin-bottom:5px;
    padding-top:0px;
    padding-bottom:0px;
    padding-left:15px;
    padding-right:15px;
    height:20px
}

pre{
    white-space: pre-wrap;  
    white-space: -moz-pre-wrap; 
    white-space: -pre-wrap; 
    white-space: -o-pre-wrap; 
    word-wrap: break-word; 
    width:100%; overflow-x:auto;
}
</style>


<div class="text-justify"><span id="chang2019cronus">Chang, H., Shejwalkar, V., Shokri, R., &amp; Houmansadr, A. (2021). Cronus: Robust and heterogeneous collaborative learning with black-box knowledge transfer. <i>NFFL at NeurIPS</i>.</span></div>

<!-- You can use the below to make your name bold -->
<!-- <span id="chang2019cronus">Chang, H., Shejwalkar, V., Shokri, R., &amp; Houmansadr, A. (2021). Cronus: Robust and heterogeneous collaborative learning with black-box knowledge transfer. <i>NFFL at NeurIPS</i>.</span></div> -->







<a href="https://arxiv.org/abs/1912.11279" target="_blank"><button class="btn btn-success btm-sm">PDF</button></a>







<button class="btn btn-warning btm-sm" onclick="toggleAbstractchang2019cronus()">ABSTRACT</button>



<button class="btn btn-danger btm-sm" onclick="toggleBibtexchang2019cronus()">BIB</button>





<div id="achang2019cronus" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>@article{chang2019cronus,
  author = {Chang, Hongyan and Shejwalkar, Virat and Shokri, Reza and Houmansadr, Amir},
  title = {Cronus: Robust and heterogeneous collaborative learning with black-box knowledge transfer},
  journaltitle = {NFFL at NeurIPS},
  date = {2021},
  doi = {1912.11279}
}
</pre>
</div>


<div id="bchang2019cronus" style="display: none; background-color:black; border-radius:5px; padding:10px; margin-bottom:20px;">
<pre>Collaborative (federated) learning enables multiple parties to train a model without sharing their private data, but through repeated sharing of the parameters of their local models. Despite its advantages, this approach has many known privacy and security weaknesses and performance overhead, in addition to being limited only to models with homogeneous architectures. Shared parameters leak a significant amount of information about the local (and supposedly private) datasets. Besides, federated learning is severely vulnerable to poisoning attacks, where some participants can adversarially influence the aggregate parameters. Large models, with high dimensional parameter vectors, are in particular highly susceptible to privacy and security attacks: curse of dimensionality in federated learning. We argue that sharing parameters is the most naive way of information exchange in collaborative learning, as they open all the internal state of the model to inference attacks, and maximize the model’s malleability by stealthy poisoning attacks. We propose Cronus, a robust collaborative machine learning framework. The simple yet effective idea behind designing Cronus is to control, unify, and significantly reduce the dimensions of the exchanged information between parties, through robust knowledge transfer between their black-box local models. We evaluate all existing federated learning algorithms against poisoning attacks, and we show that Cronus is the only secure method, due to its tight robustness guarantee. Treating local models as black-box, reduces the information leakage through models, and enables us using existing privacy-preserving algorithms that mitigate the risk of information leakage through the model’s output (predictions). Cronus also has a significantly lower sample complexity, compared to federated learning, which does not bind its security to the number of participants.</pre>
</div>

<script>
function toggleBibtexchang2019cronus(parameter) {
    var x= document.getElementById('achang2019cronus');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
function toggleAbstractchang2019cronus(parameter) {
    var x= document.getElementById('bchang2019cronus');
    if (x.style.display === 'none') {
        x.style.display = 'block';
    } else {
        x.style.display = 'none';
    }
}
</script>
</li></ol>
</div>

</div>

      </div>
    </div>

    <br/>
<section id="footer">
<div class="container-footer">
  <div class="panel-footer">
	  <div class="row">
		<div class="col-sm-4">
		    <h5>About</h5>	
            <p>Hongyan Chang<br/> National University of Singapore
</p>
		</div>

		<div class="col-sm-4">
		    <h5>Contact</h5>	
            <p><a href="https://github.com/changhongyan123"><i class="fa fa-github fa-1x"></i> Github</a>
</p>
		</div>

		<div class="col-sm-4">
		    <h5>Coordinates</h5>	
            <p>Security Lab<br/> COM3, 11 Research Link<br/> Singapore 119391
</p>
		</div>
	  </div>

      <center><p>&copy 2024 Hongyan Chang </p></center>
	</div>
  </div>
</div>

<script src="/https://www.comp.nus.edu.sg/~hongyan/assets/javascript/bootstrap/bootstrap.bundle.min.js"></script>


  </body>

</html>
